[
  {
    "id": "retail-demand-revenue-intelligence",
    "title": "Retail Demand and Revenue Intelligence",
    "tagline": "Batch retail data pipeline that turns raw orders into clear demand and revenue insight across products, stores, and time.",
    "problem": "Retail data often lands as small but messy tables across orders, items, products, and stores, making it hard to see reliable demand and revenue patterns.",
    "impact": "Provides a clean, repeatable view of daily demand and revenue by product, store, and category, forming a solid base for stock, pricing, and performance decisions.",
    "category": "pipelines",
    "roleType": "Solo",
    "roleDetail": "Designed and implemented the end-to-end batch analytics and forecasting pipeline as a portfolio project.",
    "team": "Individual portfolio project.",
    "timeline": "Portfolio project (self-paced)",
    "tools": [
      "Python",
      "SQL",
      "Pandas",
      "Scikit-learn",
      "Postgres",
      "Spark",
      "dbt",
      "Airflow",
      "Docker",
      "Power BI or Tableau"
    ],
    "links": {
      "github": "[Add GitHub link to project-1-retail-demand folder]",
      "drive": "[Add]",
      "canva": "[Add]"
    },
    "featured": true,
    "contextGoal": "Move from raw retail tables to a clean, tested, and explainable batch pipeline that supports demand and revenue analysis for non-technical stakeholders.",
    "architecture": {
      "diagram": "",
      "note": "Raw CSVs are ingested with Python into Postgres, cleaned and aggregated with Spark, modeled with dbt into gold marts, and surfaced via a simple ML baseline and BI dashboards."
    },
    "whatIBuilt": [
      "Sample retail schema with orders, order items, products, and stores for realistic structure.",
      "Python ingestion script that loads raw CSVs into Postgres as raw tables.",
      "Spark batch jobs that clean data and build silver and gold layers for daily sales.",
      "dbt models that formalise marts for demand and revenue by day, store, and product category.",
      "A simple Scikit-learn baseline model that forecasts short-term demand from the gold tables.",
      "Guidance for connecting Power BI or Tableau directly to the mart tables for dashboards."
    ],
    "challenges": [
      "Designing a schema that feels realistic but stays small enough for a portfolio demo.",
      "Keeping the batch flow readable for beginners while still showing medallion-style layers.",
      "Balancing ML complexity with the goal of a transparent, explainable baseline forecast."
    ],
    "outcomeDetails": [
      "Produces a gold-level daily sales table plus clean product and store dimensions ready for BI.",
      "Demonstrates how Python, Spark, dbt, and Scikit-learn can work together in a single batch pipeline.",
      "Shows end-to-end thinking from ingestion through modeling, forecasting, and visualisation for retail data."
    ],
    "artifacts": [
      {
        "title": "Batch Pipeline Diagram",
        "type": "image",
        "src": "",
        "caption": "High-level view from raw CSVs to gold marts and dashboards."
      },
      {
        "title": "Retail Dashboard Mock",
        "type": "image",
        "src": "",
        "caption": "Example revenue and demand views by product and store."
      }
    ]
  },
  {
    "id": "realtime-fraud-risk-signal-pipeline",
    "title": "Real-Time Fraud and Risk Signal Pipeline",
    "tagline": "Streaming-style fraud and risk simulation that turns synthetic events into rolling user features and simple risk signals.",
    "problem": "Fraud and risk teams struggle to build and test live-style pipelines when they only have static data and no safe place to experiment.",
    "impact": "Provides a local, replayable environment to explore rolling features, event windows, and nightly retraining loops for risk scoring.",
    "category": "orchestration",
    "roleType": "Solo",
    "roleDetail": "Designed and implemented the streaming-style simulation, feature pipeline, and nightly training scaffold as a portfolio project.",
    "team": "Individual portfolio project.",
    "timeline": "Portfolio project (self-paced)",
    "tools": [
      "Python",
      "SQL",
      "Pandas",
      "Scikit-learn",
      "Spark Structured Streaming",
      "Postgres",
      "dbt",
      "Airflow",
      "Docker"
    ],
    "links": {
      "github": "[Add GitHub link to project-2-realtime-fraud folder]",
      "drive": "[Add]",
      "canva": "[Add]"
    },
    "featured": true,
    "contextGoal": "Simulate a real-time fraud monitoring flow where synthetic transaction events are turned into five-minute rolling features and simple risk marts for monitoring and modelling.",
    "architecture": {
      "diagram": "",
      "note": "A Python event generator drops JSON transactions into a stream inbox; Spark Structured Streaming reads them into bronze and silver tables, builds rolling user features, stores them in Postgres, and dbt creates marts for risk monitoring and nightly ML training."
    },
    "whatIBuilt": [
      "Synthetic transaction event generator that writes realistic JSON batches into a stream inbox folder.",
      "Spark Structured Streaming job that ingests events into bronze and silver layers in Postgres.",
      "Five-minute rolling feature tables per user, including transaction counts and average amounts.",
      "dbt models that turn raw and feature tables into risk monitoring marts for analysts.",
      "A Scikit-learn baseline training script that reads recent data and produces a simple model report.",
      "Airflow DAG scaffolding for nightly feature backfill, training, and report generation."
    ],
    "challenges": [
      "Designing a streaming-style pipeline that can run entirely on a local laptop without Kafka.",
      "Keeping the structured streaming job simple enough for beginners but still realistic.",
      "Connecting streaming features, dbt marts, and nightly ML training into one coherent story."
    ],
    "outcomeDetails": [
      "Demonstrates how event-style data can be turned into near real-time features for fraud and risk monitoring.",
      "Shows integration of Spark Structured Streaming, Postgres, dbt, and Scikit-learn in a single project.",
      "Provides a safe sandbox for explaining rolling windows, user-level features, and nightly model retraining."
    ],
    "artifacts": [
      {
        "title": "Streaming Topology Sketch",
        "type": "image",
        "src": "",
        "caption": "Logical flow from event generator through streaming jobs to feature marts."
      },
      {
        "title": "Risk Monitoring Mart Snapshot",
        "type": "image",
        "src": "",
        "caption": "Example aggregated metrics by country, merchant, and time window."
      }
    ]
  },
  {
    "id": "modern-analytics-engineering-showcase",
    "title": "Modern Analytics Engineering Showcase",
    "tagline": "Focused analytics engineering project that turns raw tables into clean, tested, and well-documented marts with dbt.",
    "problem": "Teams often have raw data and dashboards but lack a clear, documented semantic layer between them.",
    "impact": "Provides a small, opinionated dbt project that demonstrates layered models, tests, and documentation for sales, marketing, and support data.",
    "category": "analytics",
    "roleType": "Solo",
    "roleDetail": "Owned the ingestion pattern, dbt model design, and documentation as a focused analytics engineering portfolio project.",
    "team": "Individual portfolio project.",
    "timeline": "Portfolio project (self-paced)",
    "tools": [
      "SQL",
      "dbt",
      "Python",
      "Pandas",
      "Postgres",
      "Docker",
      "Airflow",
      "Power BI or Tableau"
    ],
    "links": {
      "github": "[Add GitHub link to project-3-analytics-engineering folder]",
      "drive": "[Add]",
      "canva": "[Add]"
    },
    "featured": true,
    "contextGoal": "Show how dbt and SQL can create a clean semantic layer that supports sales, marketing, and support analysis with tests and documentation.",
    "architecture": {
      "diagram": "",
      "note": "Raw CSVs are loaded into Postgres with Python, then dbt builds layered models (staging, intermediate, marts) that power BI dashboards and self-service queries."
    },
    "whatIBuilt": [
      "Compact sample datasets for sales, marketing campaigns, and support tickets.",
      "Python and Pandas ingestion script that lands raw tables in Postgres.",
      "dbt staging models that clean and standardise column names, types, and basic rules.",
      "Intermediate dbt models that join domains to answer cross-functional questions.",
      "Mart models that expose ready-to-use tables for sales trends, marketing efficiency, and support outcomes.",
      "Basic tests and documentation inside dbt so analysts can trust and understand each model."
    ],
    "challenges": [
      "Designing simple but realistic schemas for three different business domains.",
      "Keeping model layers small while still showing the value of staging, intermediate, and marts.",
      "Writing documentation that helps non-technical stakeholders understand what each table means."
    ],
    "outcomeDetails": [
      "Shows a full analytics engineering pattern with dbt, from raw tables to documented marts.",
      "Demonstrates how a semantic layer can support BI tools and self-service SQL safely.",
      "Acts as a clean, teachable reference for talking about dbt, tests, and documentation in interviews."
    ],
    "artifacts": [
      {
        "title": "dbt Model DAG Screenshot",
        "type": "image",
        "src": "",
        "caption": "Graph of staging, intermediate, and mart models for the project."
      },
      {
        "title": "BI Mock Dashboard",
        "type": "image",
        "src": "",
        "caption": "Example combined view of sales, marketing, and support metrics."
      }
    ]
  }
]
