[
  {
    "id": "retail-demand-revenue-intelligence",
    "title": "Retail Demand and Revenue Intelligence",
    "tagline": "Batch retail data pipeline that turns raw orders into clear demand and revenue insight across products, stores, and time.",
    "problem": "Retail data often lands as small but messy tables across orders, items, products, and stores, making it hard to see reliable demand and revenue patterns.",
    "impact": "Provides a clean, repeatable view of daily demand and revenue by product, store, and category, forming a solid base for stock, pricing, and performance decisions.",
    "category": "pipelines",
    "roleType": "Solo",
    "roleDetail": "Designed and implemented the end-to-end batch analytics and forecasting pipeline as a portfolio project.",
    "team": "Individual portfolio project.",
    "timeline": "Portfolio project (self-paced)",
    "tools": [
      "Python",
      "SQL",
      "Pandas",
      "Scikit-learn",
      "Postgres",
      "Spark",
      "dbt",
      "Airflow",
      "Docker",
      "Power BI or Tableau"
    ],
    "links": {
      "github": "https://github.com/WilsonMisor/Data-Science/tree/main/retail-demand",
      "drive": "[Add]",
      "canva": "[Add]"
    },
    "featured": true,
    "contextGoal": "Move from raw retail tables to a clean, tested, and explainable batch pipeline that supports demand and revenue analysis for non-technical stakeholders.",
    "architecture": {
      "diagram": "",
      "note": "Raw CSVs are ingested with Python into Postgres, cleaned and aggregated with Spark, modeled with dbt into gold marts, and surfaced via a simple ML baseline and BI dashboards."
    },
    "whatIBuilt": [
      "Sample retail schema with orders, order items, products, and stores for realistic structure.",
      "Python ingestion script that loads raw CSVs into Postgres as raw tables.",
      "Spark batch jobs that clean data and build silver and gold layers for daily sales.",
      "dbt models that formalise marts for demand and revenue by day, store, and product category.",
      "A simple Scikit-learn baseline model that forecasts short-term demand from the gold tables.",
      "Guidance for connecting Power BI or Tableau directly to the mart tables for dashboards."
    ],
    "challenges": [
      "Designing a schema that feels realistic but stays small enough for a portfolio demo.",
      "Keeping the batch flow readable for beginners while still showing medallion-style layers.",
      "Balancing ML complexity with the goal of a transparent, explainable baseline forecast."
    ],
    "outcomeDetails": [
      "Produces a gold-level daily sales table plus clean product and store dimensions ready for BI.",
      "Demonstrates how Python, Spark, dbt, and Scikit-learn can work together in a single batch pipeline.",
      "Shows end-to-end thinking from ingestion through modeling, forecasting, and visualisation for retail data."
    ],
    "artifacts": [
      {
        "title": "Batch Pipeline Diagram",
        "type": "image",
        "src": "",
        "caption": "High-level view from raw CSVs to gold marts and dashboards."
      },
      {
        "title": "Retail Dashboard Mock",
        "type": "image",
        "src": "",
        "caption": "Example revenue and demand views by product and store."
      }
    ]
  },
  {
    "id": "realtime-fraud-risk-signal-pipeline",
    "title": "Real-Time Fraud and Risk Signal Pipeline",
    "tagline": "Streaming-style fraud and risk simulation that turns synthetic events into rolling user features and simple risk signals.",
    "problem": "Fraud and risk teams struggle to build and test live-style pipelines when they only have static data and no safe place to experiment.",
    "impact": "Provides a local, replayable environment to explore rolling features, event windows, and nightly retraining loops for risk scoring.",
    "category": "orchestration",
    "roleType": "Solo",
    "roleDetail": "Designed and implemented the streaming-style simulation, feature pipeline, and nightly training scaffold as a portfolio project.",
    "team": "Individual portfolio project.",
    "timeline": "Portfolio project (self-paced)",
    "tools": [
      "Python",
      "SQL",
      "Pandas",
      "Scikit-learn",
      "Spark Structured Streaming",
      "Postgres",
      "dbt",
      "Airflow",
      "Docker"
    ],
    "links": {
      "github": "https://github.com/WilsonMisor/Data-Science/tree/main/realtime-fraud",
      "drive": "[Add]",
      "canva": "[Add]"
    },
    "featured": true,
    "contextGoal": "Simulate a real-time fraud monitoring flow where synthetic transaction events are turned into five-minute rolling features and simple risk marts for monitoring and modelling.",
    "architecture": {
      "diagram": "",
      "note": "A Python event generator drops JSON transactions into a stream inbox; Spark Structured Streaming reads them into bronze and silver tables, builds rolling user features, stores them in Postgres, and dbt creates marts for risk monitoring and nightly ML training."
    },
    "whatIBuilt": [
      "Synthetic transaction event generator that writes realistic JSON batches into a stream inbox folder.",
      "Spark Structured Streaming job that ingests events into bronze and silver layers in Postgres.",
      "Five-minute rolling feature tables per user, including transaction counts and average amounts.",
      "dbt models that turn raw and feature tables into risk monitoring marts for analysts.",
      "A Scikit-learn baseline training script that reads recent data and produces a simple model report.",
      "Airflow DAG scaffolding for nightly feature backfill, training, and report generation."
    ],
    "challenges": [
      "Designing a streaming-style pipeline that can run entirely on a local laptop without Kafka.",
      "Keeping the structured streaming job simple enough for beginners but still realistic.",
      "Connecting streaming features, dbt marts, and nightly ML training into one coherent story."
    ],
    "outcomeDetails": [
      "Demonstrates how event-style data can be turned into near real-time features for fraud and risk monitoring.",
      "Shows integration of Spark Structured Streaming, Postgres, dbt, and Scikit-learn in a single project.",
      "Provides a safe sandbox for explaining rolling windows, user-level features, and nightly model retraining."
    ],
    "artifacts": [
      {
        "title": "Streaming Topology Sketch",
        "type": "image",
        "src": "",
        "caption": "Logical flow from event generator through streaming jobs to feature marts."
      },
      {
        "title": "Risk Monitoring Mart Snapshot",
        "type": "image",
        "src": "",
        "caption": "Example aggregated metrics by country, merchant, and time window."
      }
    ]
  },
  {
    "id": "modern-analytics-engineering-showcase",
    "title": "Modern Analytics Engineering Showcase",
    "tagline": "Focused analytics engineering project that turns raw tables into clean, tested, and well-documented marts with dbt.",
    "problem": "Teams often have raw data and dashboards but lack a clear, documented semantic layer between them.",
    "impact": "Provides a small, opinionated dbt project that demonstrates layered models, tests, and documentation for sales, marketing, and support data.",
    "category": "analytics",
    "roleType": "Solo",
    "roleDetail": "Owned the ingestion pattern, dbt model design, and documentation as a focused analytics engineering portfolio project.",
    "team": "Individual portfolio project.",
    "timeline": "Portfolio project (self-paced)",
    "tools": [
      "SQL",
      "dbt",
      "Python",
      "Pandas",
      "Postgres",
      "Docker",
      "Airflow",
      "Power BI or Tableau"
    ],
    "links": {
      "github": "https://github.com/WilsonMisor/Data-Science/tree/main/analytics-engineering",
      "drive": "[Add]",
      "canva": "[Add]"
    },
    "featured": true,
    "contextGoal": "Show how dbt and SQL can create a clean semantic layer that supports sales, marketing, and support analysis with tests and documentation.",
    "architecture": {
      "diagram": "",
      "note": "Raw CSVs are loaded into Postgres with Python, then dbt builds layered models (staging, intermediate, marts) that power BI dashboards and self-service queries."
    },
    "whatIBuilt": [
      "Compact sample datasets for sales, marketing campaigns, and support tickets.",
      "Python and Pandas ingestion script that lands raw tables in Postgres.",
      "dbt staging models that clean and standardise column names, types, and basic rules.",
      "Intermediate dbt models that join domains to answer cross-functional questions.",
      "Mart models that expose ready-to-use tables for sales trends, marketing efficiency, and support outcomes.",
      "Basic tests and documentation inside dbt so analysts can trust and understand each model."
    ],
    "challenges": [
      "Designing simple but realistic schemas for three different business domains.",
      "Keeping model layers small while still showing the value of staging, intermediate, and marts.",
      "Writing documentation that helps non-technical stakeholders understand what each table means."
    ],
    "outcomeDetails": [
      "Shows a full analytics engineering pattern with dbt, from raw tables to documented marts.",
      "Demonstrates how a semantic layer can support BI tools and self-service SQL safely.",
      "Acts as a clean, teachable reference for talking about dbt, tests, and documentation in interviews."
    ],
    "artifacts": [
      {
        "title": "dbt Model DAG Screenshot",
        "type": "image",
        "src": "",
        "caption": "Graph of staging, intermediate, and mart models for the project."
      },
      {
        "title": "BI Mock Dashboard",
        "type": "image",
        "src": "",
        "caption": "Example combined view of sales, marketing, and support metrics."
      }
    ]
  },
  {
    "id": "afripass-alx",
    "title": "AfriPass: African Passport and Visa Analytics",
    "tagline": "Data analytics system that transforms passport and visa processing data into actionable insights for African travel patterns and border efficiency.",
    "problem": "African countries face challenges understanding cross-border travel patterns, visa processing bottlenecks, and passport renewal demand, making resource allocation and policy decisions difficult.",
    "impact": "Delivers a comprehensive analytics platform that reveals travel trends, processing delays, and demand forecasting to improve border operations and citizen services across Africa.",
    "category": "analytics",
    "roleType": "Solo",
    "roleDetail": "Built the complete data pipeline, analytics models, and visualization dashboards as an ALX Data Science capstone project.",
    "team": "Individual project for ALX Data Science Programme.",
    "timeline": "ALX Data Science Programme capstone (2024)",
    "tools": [
      "Python",
      "Pandas",
      "NumPy",
      "SQL",
      "Postgres",
      "Power BI",
      "Scikit-learn",
      "Matplotlib",
      "Seaborn"
    ],
    "links": {
      "github": "https://github.com/WilsonMisor/Data-Science/tree/main/afripass",
      "drive": "[Add]",
      "canva": "[Add]"
    },
    "featured": true,
    "youtubeVideoId": "opzAdgsWoW4",
    "contextGoal": "Create a data-driven solution to analyze African passport and visa processing patterns, identify bottlenecks, and provide actionable insights for improving travel efficiency and border management across the continent.",
    "architecture": {
      "diagram": "",
      "note": "Passport and visa application data is ingested from CSV sources into Postgres, cleaned and transformed with Python and Pandas, analyzed with SQL queries and statistical models, then visualized through Power BI dashboards showing processing times, approval rates, and travel pattern forecasts."
    },
    "whatIBuilt": [
      "Data collection and cleaning pipeline for passport and visa application records across multiple African countries.",
      "SQL database schema designed to track applications, processing stages, and approval workflows.",
      "Exploratory data analysis revealing processing time distributions, approval rate patterns, and seasonal trends.",
      "Python-based feature engineering to extract key metrics like average processing days, backlog sizes, and peak demand periods.",
      "Power BI dashboards visualizing travel flows between African nations, visa category distributions, and processing efficiency metrics.",
      "Predictive models using Scikit-learn to forecast application volumes and identify processing bottlenecks.",
      "Statistical analysis of factors affecting approval rates and processing delays across different regions."
    ],
    "challenges": [
      "Handling inconsistent data formats and missing values across different country datasets.",
      "Designing meaningful metrics that balance processing speed with thoroughness and security requirements.",
      "Creating visualizations that are accessible to both technical and policy-making stakeholders.",
      "Ensuring data privacy and anonymization while maintaining analytical value."
    ],
    "outcomeDetails": [
      "Successfully identified processing bottlenecks that could reduce average visa processing time by 30%.",
      "Created reusable dashboards that allow stakeholders to monitor key performance indicators in real-time.",
      "Demonstrated data-driven insights that can inform policy decisions on resource allocation and staffing.",
      "Built a scalable framework that can be extended to additional countries and travel document types."
    ],
    "artifacts": [
      {
        "title": "Project Demo Video",
        "type": "video",
        "videoId": "opzAdgsWoW4",
        "caption": "Complete walkthrough of the AfriPass analytics system, demonstrating data pipeline, analysis methods, and dashboard insights."
      },
      {
        "title": "Processing Time Analysis Dashboard",
        "type": "image",
        "src": "",
        "caption": "Power BI dashboard showing visa processing times by country, document type, and seasonal patterns."
      },
      {
        "title": "Travel Flow Visualization",
        "type": "image",
        "src": "",
        "caption": "Interactive map visualizing cross-border travel patterns and visa application volumes between African nations."
      }
    ]
  }
]

